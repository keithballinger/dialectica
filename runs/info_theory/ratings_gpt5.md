1) Score: 6/10 — Major Revisions: Novelty 9, Falsifiability 7, Feasibility 4; strong and appealing universal finite-blocklength law for learned codes is testable but proving dispersion matching with a vanishing representation penalty likely requires restrictive assumptions.
2) Score: 5/10 — Reject: Novelty 9, Falsifiability 6, Feasibility 3; an exact prior-free MI frontier for (ε,δ)-DP with staircase optimality conflicts with known dependence on data/domain and seems unlikely to admit a closed form.
3) Score: 6/10 — Major Revisions: Novelty 7, Falsifiability 5, Feasibility 5; links to Landauer and thermodynamics of prediction are compelling but similar results exist and experimental verification at nanoscale is arduous.
4) Score: 4/10 — Reject: Novelty 6, Falsifiability 9, Feasibility 3; claiming MI SDPI depends only on the channel’s zero pattern contradicts known sensitivity to weights (beyond chi-square cases), so counterexamples are likely.
5) Score: 5/10 — Major Revisions: Novelty 8, Falsifiability 6, Feasibility 4; a universal lower bound via directed information is intriguing but the sign/expectation issues and dependence on training noise make a general theorem doubtful.
6) Score: 7/10 — Minor Revisions: Novelty 8, Falsifiability 8, Feasibility 6; a sharp structural iff condition for superadditivity in deterministic FSCs with correlated encoding is plausible and testable, though proofs will be technically heavy.
7) Score: 6/10 — Major Revisions: Novelty 7, Falsifiability 5, Feasibility 6; the algorithmic-statistics link is interesting but finite-block universal constants and empirical tests via compression surrogates may be too weak without tighter theory.
8) Score: 6/10 — Major Revisions: Novelty 9, Falsifiability 8, Feasibility 3; an operationally unique, nonnegative PID via guessing would be a breakthrough, yet satisfying all debated axioms and proving uniqueness is very challenging.
9) Score: 8/10 — Minor Revisions: Novelty 7, Falsifiability 8, Feasibility 7; a 1/r per-view rate law under conditional independence aligns with CEO/distributed IB intuitions and seems provable with clear assumptions and simulations.
10) Score: 7/10 — Minor Revisions: Novelty 8, Falsifiability 7, Feasibility 6; the spectral criterion and chi-squared affinity scaling could unify theory for variational f-divergence estimators, though careful specification of function classes is needed.

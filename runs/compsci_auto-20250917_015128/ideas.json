{"ideas":[{"index":1,"title":"A Token-Diversity Threshold Governs In-Context Learning Emergence","summary":"There exists a critical ratio of unique token types to model parameters that predicts when in-context learning appears across tasks.","layperson":"As language models see more varied words and symbols during training, they suddenly get better at learning from examples placed in the prompt. This idea says there is a tipping point where enough variety unlocks this behavior. We can estimate this tipping point from just how many different tokens the model saw and its size.","falsification":"Train families of models across controlled synthetic corpora that vary token-type counts while holding total tokens and architectures fixed; for multiple in-context tasks, measure the emergence point (e.g., accuracy jump) and test whether a single token-diversity/parameter ratio predicts it across settings. If no consistent threshold or scaling function fits across tasks and sizes, the theory is falsified.","ibm_cost_plan":"n/a","novelty":"Prior scaling laws focus on data/parameter counts but do not posit a specific token-diversity-to-parameter threshold for in-context learning."},{"index":2,"title":"Rare Sparse Features Enable Tool Use Compositionality","summary":"Tool-use ability in LLMs is driven by a small set of rare, sparsely activated features whose removal disrupts tool composition without large perplexity changes.","layperson":"Models can call tools like calculators or web search and combine them to solve tasks. This idea says a few specialized switches inside the model are crucial for coordinating such tools, even if most of the model looks unaffected. If we snip just those switches, tool use breaks while general language ability stays similar.","falsification":"Identify sparse features via activation sparsity and attribution during tool-use prompts; perform targeted ablations or reinitializations and measure drop in tool-use and tool-composition benchmarks versus minimal change in perplexity and general QA. If performance degrades uniformly or requires widespread ablation to fail, the theory is falsified.","ibm_cost_plan":"n/a","novelty":"Connects tool-use compositionality specifically to rare sparse features rather than distributed representations generally."},{"index":3,"title":"Self-Consistency Approximates Bayesian Model Averaging Under Logit Noise","summary":"Majority voting over multiple chain-of-thought samples approximates Bayesian model averaging if sampling injects calibrated logit noise.","layperson":"When models answer step-by-step multiple times and we pick the most common answer, accuracy often improves. This theory says that trick works like averaging many slightly noisy versions of the model, similar to a classic Bayesian method. If we add controlled noise to the model’s scores, the voting should match the Bayesian average.","falsification":"Inject controlled Gaussian noise into logits at decode time, vary noise temperature, and compare self-consistency votes to predictions from explicit Bayesian averaging over a learned Gaussian perturbation model; if no regime yields close agreement on accuracy and per-item probabilities, falsify.","ibm_cost_plan":"n/a","novelty":"Provides a concrete noise model linking self-consistency to Bayesian averaging with testable quantitative equivalence."},{"index":4,"title":"Instruction Tuning Is a Low-Rank Preference Operator","summary":"The changes caused by instruction tuning can be captured by a bounded-rank update that aligns with a learned preference gradient.","layperson":"Fine-tuning a model on instructions seems big, but this theory claims it mostly nudges the model along a few key directions. Those directions encode human preferences about how answers should look. If a small low-rank patch reproduces full fine-tuning, the idea holds.","falsification":"Train a fully fine-tuned instruction model and fit LoRA adapters at varying ranks to match it from the base; if low ranks (below a predicted bound from data dimension) cannot recover behavior on diverse instruction benchmarks and preference metrics, falsify.","ibm_cost_plan":"n/a","novelty":"Posits an explicit low-rank structure tied to preference gradients rather than generic parameter efficiency."},{"index":5,"title":"Burstiness Controls Power-Law Context Extrapolation","summary":"A model’s ability to handle longer contexts than seen in training follows a power law determined by training-data burstiness (long contiguous spans).","layperson":"Models trained on longer uninterrupted stretches of text learn better long-range memory. This theory says the pattern of long stretches during training predicts exactly how well the model handles even longer inputs, following a simple curve. By changing that pattern, we should change the curve’s slope.","falsification":"Generate corpora with matched tokens but different burstiness profiles; train identical models and measure loss/accuracy at extrapolated context lengths, fitting power-law exponents; if exponents do not systematically vary with burstiness metrics, falsify.","ibm_cost_plan":"n/a","novelty":"Links a measurable corpus property (burstiness) to a specific power-law of context extrapolation."},{"index":6,"title":"RLHF Reorganizes Representations into Linearly Decodable Subskills","summary":"Reinforcement learning from human feedback increases linear separability of latent subskills without necessarily improving base task encoders.","layperson":"After aligning models with human feedback, certain abilities (like being helpful or safe) become easier to read out from internal activations. The base knowledge may not change much, but how it’s arranged does. We can test this by training simple classifiers on hidden states before and after alignment.","falsification":"Collect hidden states on labeled multi-attribute datasets pre/post-RLHF and train linear probes; if separability (AUC) for targeted subskills does not increase while base-language probes remain similar, or improvements vanish under representation permutation tests, falsify.","ibm_cost_plan":"n/a","novelty":"Shifts focus from capability gains to a testable representational reorganization hypothesis post-RLHF."},{"index":7,"title":"Tokenization Entropy Predicts Hallucination","summary":"Entities and facts with higher tokenization entropy induce higher hallucination rates during generation.","layperson":"Some words or names are chopped into many subpieces by tokenizers, making them harder for models to handle. This idea says those awkwardly tokenized items cause more made-up facts. By measuring how unpredictably text splits into tokens, we can predict when the model will hallucinate.","falsification":"Compute tokenization-entropy metrics for entities and prompts; run controlled fact-checking generation and regress hallucination rates on entropy while controlling for frequency and context; if entropy adds no predictive power across models/tokenizers, falsify.","ibm_cost_plan":"n/a","novelty":"Proposes a specific, computable tokenization-based predictor of hallucination beyond frequency and rarity."},{"index":8,"title":"Retrieval Suppresses Grokking via Alternative Information Channels","summary":"Providing retrieval during training reduces grokking delays on algorithmic tasks by supplying an external mutual-information path.","layperson":"Small models sometimes suddenly “get it” after long training, a weird effect called grokking. This theory says giving the model a lookup tool during training helps it learn sooner by providing another route to the needed information. If retrieval shortens the delay reliably, the idea is supported.","falsification":"Train models on synthetic algorithmic tasks with/without retrieval-augmented context under matched tokens; measure time-to-generalization and mutual information between inputs and retrieved snippets; if grokking delay is unaffected or worsens, falsify.","ibm_cost_plan":"n/a","novelty":"Connects retrieval augmentation to the specific phenomenon of grokking via an information-theoretic mechanism."},{"index":9,"title":"Gradient Alignment Predicts Few-Shot Transfer","summary":"Cosine similarity between task gradients computed via low-rank adapters predicts few-shot transfer gains between tasks.","layperson":"If learning one task nudges the model in a direction similar to another task, the model should transfer better with few examples. We can measure these directions by looking at how the model’s weights want to change for each task. Better alignment should mean better transfer.","falsification":"Estimate per-task gradient directions using LoRA on small batches; compute pairwise cosine similarities and correlate with few-shot performance improvements; if no significant correlation holds across datasets and models, falsify.","ibm_cost_plan":"n/a","novelty":"Offers a practical, code-computable predictor of transfer using approximate gradients rather than post-hoc performance only."},{"index":10,"title":"Training Temperature Sets the Depth of Logit-Lens Linearity","summary":"The depth at which intermediate layer outputs linearly predict final logits scales monotonically with the sampling temperature used during training.","layperson":"A simple “logit lens” can peek into layers and guess the model’s final answer partway through. This theory says how deep you must go before that guess works well depends on the temperature used while training: higher temperatures push clear predictions deeper. Changing temperature should shift where the lens works best.","falsification":"Train identical models at varying softmax temperatures or with label-smoothing equivalents; compute layer-wise KL between logit-lens predictions and final logits, and test for monotonic shifts in the optimal layer; if no consistent relation appears, falsify.","ibm_cost_plan":"n/a","novelty":"Links a controllable training hyperparameter to a measurable breakpoint in logit-lens linearity across depth."}]}

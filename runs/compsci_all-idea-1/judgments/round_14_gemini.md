Publish

The manuscript presents a novel, theoretically-grounded, and practical method for denoising quantized logits in LLM inference. The revision is exemplary, having systematically addressed all prior concerns. The method's placement within the decoding pipeline is now explicit, and potential issues like quantization bias and domain shift are handled with principled, optional mechanisms (magnitude-conditioned banding, online adaptation). The experimental plan is a model of rigor, designed for falsification with strong baselines, comprehensive ablations, and latency-aware evaluation. The inclusion of pseudocode, detailed implementation notes, and a reproducibility checklist makes the work exceptionally clear and verifiable. The paper is a significant contribution to efficient LLM inference and is ready for publication.

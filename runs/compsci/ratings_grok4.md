1) Score: Novelty 9/10, Falsifiability 9/10, Feasibility 8/10 — This idea introduces a fundamental entropy-based bound on branch prediction, novel in its architecture-independence, falsifiable via trace analysis and predictor benchmarks, and feasible for publication in top architecture journals like ISCA with solid experiments.
2) Score: Novelty 8/10, Falsifiability 8/10, Feasibility 7/10 — Linking code comprehension to graph entropy offers a fresh quantitative model, testable through human studies across languages, but feasibility is moderate due to challenges in conducting reliable eye-tracking experiments for leading SE/HCI venues.
3) Score: Novelty 9/10, Falsifiability 9/10, Feasibility 9/10 — Proposing an architecture-invariant scaling law based on manifold dimension extends existing ML theories innovatively, with clear falsification via diverse model training, highly feasible for NeurIPS/ICML given empirical tractability.
4) Score: Novelty 8/10, Falsifiability 8/10, Feasibility 8/10 — Mapping side-channel success to channel capacity provides a novel quantitative framework, falsifiable through hardware experiments and attacks, feasible for security conferences like USENIX with measured capacities across platforms.
5) Score: Novelty 9/10, Falsifiability 8/10, Feasibility 8/10 — Tying tail latency to spectral gaps in service graphs is a creative shift from traditional metrics, testable in controlled topologies, feasible for systems journals like NSDI if experiments robustly bound variability.
6) Score: Novelty 8/10, Falsifiability 9/10, Feasibility 9/10 — Establishing equivalence between test-time adaptation and adaptive kernels unifies methods neatly, easily falsifiable by comparing predictions on datasets, highly feasible for ML conferences with synthetic and real data validations.
7) Score: Novelty 9/10, Falsifiability 8/10, Feasibility 9/10 — Framing in-context learning as a percolation phase transition is highly original, falsifiable via sparse Transformer training, feasible for top ML venues like ICLR with controlled sparsity experiments.
8) Score: Novelty 8/10, Falsifiability 9/10, Feasibility 8/10 — Bounding AD memory by pathwidth connects to graph theory innovatively, straightforward to falsify on benchmark graphs, feasible for PL/ML journals if optimal schedules are implemented effectively.
9) Score: Novelty 8/10, Falsifiability 8/10, Feasibility 9/10 — Predicting zero-shot synthesis via grammar entropy shifts focus to formal priors, testable by inducing grammars and correlating success, highly feasible for AI/code generation tracks in conferences like ACL or NeurIPS.
10) Score: Novelty 9/10, Falsifiability 8/10, Feasibility 9/10 — Using sectional curvature to predict steerability introduces geometric insights to interpretability, falsifiable through embedding metrics and interventions, feasible for leading ML journals with curvature estimations across models.

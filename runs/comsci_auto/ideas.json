{"ideas":[{"index":1,"title":"Diversity-Per-Parameter Law for Small LLMs","summary":"For small LLMs, downstream performance scales with instruction diversity per parameter more strongly than with raw dataset size.","layperson":"When a model is small, giving it a wider variety of examples matters more than simply giving it more of the same. Think of learning a language: a pocket dictionary with many different words can beat a longer book of repetitive sentences. This claims variety per unit of model size is the key driver.","falsification":"Create multiple instruction-tuning sets with equal token counts but systematically varied diversity (e.g., by cluster count/entropy over embeddings). Finetune the same small LLM (e.g., 1–7B) on each set with matched hyperparameters and compute. Evaluate across multiple benchmarks (MMLU, GSM8K, TruthfulQA) and test whether diversity-per-parameter correlates better with performance than size-per-parameter; failure to observe this across seeds falsifies.","ibm_cost_plan":"Implement dataset generators, clustering/entropy metrics, and a training harness; total compute fits on commodity GPUs since models are small.","novelty":"Proposes a quantitative scaling relation tying dataset diversity to model parameter count specifically for small LLMs."},{"index":2,"title":"Evenly Distributed Adapters Beat Concentrated Adapters","summary":"With a fixed adapter budget, spreading small adapters across all layers yields better generalization than concentrating them in a few layers for small LLMs.","layperson":"Small add-on modules (adapters) let a model learn new tasks quickly. This idea says sprinkling small adapters throughout the model works better than piling them into just the first or last few layers. It’s like adding small upgrades to every gear rather than one big upgrade to a single gear.","falsification":"Implement LoRA/IA3 with constant total trainable parameters but different placements: early-only, middle-only, late-only, and uniform-across-layers. Finetune a small LLM on an instruction dataset, then test on out-of-domain tasks; use paired statistical tests across seeds to check if uniform placement significantly outperforms alternatives; if not, falsify.","ibm_cost_plan":"Code simple placement strategies and train on standard open datasets with 1–7B models using a few GPUs.","novelty":"Establishes a general placement principle for adapters under a strict parameter budget in small LLMs."},{"index":3,"title":"Learned Predictive KV-Cache Eviction Outperforms LRU","summary":"A tiny learned predictor of future token importance enables better KV-cache eviction than recency-based policies in small LLMs under memory constraints.","layperson":"When a model has limited memory, it must choose which past words to remember. Instead of forgetting the oldest words, this proposes a small helper module that predicts which words will matter next and keeps those.","falsification":"Train a lightweight predictor (e.g., linear/MLP) on attention traces to estimate each token’s future contribution; plug it into a custom KV eviction policy. Benchmark perplexity and task scores on long-context datasets at fixed cache sizes versus LRU/SLIDINGWINDOW/H2O; if no consistent gains across models/datasets, the claim is falsified.","ibm_cost_plan":"Collect attention stats, train tiny predictors, and integrate into decoding loops; test on long-context corpora with 1–3B models.","novelty":"Introduces a learned, forward-looking token-importance predictor for KV eviction tailored to small LLMs."},{"index":4,"title":"Learned Dithered Quantization Improves Small-LLM Calibration","summary":"Injecting learned, per-channel dither during quantization improves factuality and calibration of small LLMs at fixed bit-widths.","layperson":"Compressing a model to use fewer bits usually hurts accuracy. This idea adds a carefully tuned tiny noise during compression that makes the errors more harmless, improving truthfulness and confidence.","falsification":"Implement per-channel quantization with trainable dither scales estimated on a small calibration set (no weight updates to the base model). Compare against INT8/INT4 baselines (e.g., GPTQ/AWQ/SmoothQuant) on calibration (ECE/Brier) and factual benchmarks (TruthfulQA, MMLU). If improvements are not significant across seeds, falsify.","ibm_cost_plan":"Extend existing quantization code with learnable dither parameters and run lightweight evaluation scripts.","novelty":"Applies learned dithering to LLM weight/activation quantization for calibration/factual gains specifically in small models."},{"index":5,"title":"Progressive Context Curriculum Enables Length Extrapolation","summary":"Gradually increasing context length during finetuning yields better long-context performance in small LLMs than training at maximum length from the start under fixed tokens.","layperson":"Teaching long documents all at once overwhelms small models. Starting with short texts and slowly increasing length helps them handle long inputs later, even with the same total training effort.","falsification":"Compare schedules: (A) always-long, (B) progressive length (e.g., 512→1k→2k→4k), and (C) shuffled lengths, with identical total tokens and compute. Evaluate on long-context tasks (Needle-in-a-Haystack, LongBench) and measure perplexity vs. context; if progressive does not outperform, falsify.","ibm_cost_plan":"Implement data loader controlling sequence lengths and train small models with rope/pos-emb settings; evaluate with public long-context suites.","novelty":"Frames a controlled curriculum principle for context length in small LLMs under strict token budgets."},{"index":6,"title":"Stable Cross-Task Lottery Subnetworks in Small LLMs","summary":"There exist sparse subnetworks that recur across random seeds and transfer across tasks in small LLMs when heavily pruned.","layperson":"Inside a big network, there may be a smaller “core” that does most of the work. This claims that, for small models, the same core wiring appears again and again and can handle multiple tasks even after pruning.","falsification":"Train multiple identical small LLMs from different seeds; prune to the same sparsity using magnitude or movement pruning; compute mask overlap and test transfer by swapping masks between models/tasks. If masks are not significantly similar and do not transfer better than chance, falsify.","ibm_cost_plan":"Use pruning libraries, run few-shot finetunes, and measure overlaps/accuracy; compute-friendly with 1–3B models.","novelty":"Posits seed-stable, task-transferable sparse subnetworks uniquely prominent in small LLM regimes."},{"index":7,"title":"Optimal Self-Consistency Temperature Schedules for Small LLMs","summary":"For small LLMs, pass@k under a compute budget is maximized by a simple decreasing temperature schedule rather than a fixed temperature.","layperson":"When asking a model to try several answers and pick the best, how much randomness you use matters. This proposes that starting a bit random and cooling down over attempts gives the best results for small models given limited tries.","falsification":"Implement fixed-T baselines vs. decreasing temperature schedules with the same sample budget on code/math benchmarks (HumanEval, MBPP, GSM8K). If no consistent gains across k and seeds, or if fixed-T dominates, falsify.","ibm_cost_plan":"Add a few lines to decoding to vary temperature across samples; run standard evaluation harnesses.","novelty":"Introduces a principled, compute-aware temperature schedule tailored to small LLM self-consistency."},{"index":8,"title":"Retrieval Gains Scale Inversely with Model Size (Log-Linear Law)","summary":"The marginal improvement from retrieval augmentation increases approximately log-linearly as small LLM parameter count decreases.","layperson":"Looking things up helps smaller models more than bigger ones. This claims a predictable pattern: as the model gets smaller, retrieval adds a steadily larger boost.","falsification":"Build a unified RAG pipeline and evaluate identical retrieval settings across a size sweep (e.g., 0.5B, 1B, 3B, 7B) on QA and reasoning tasks. Fit improvement vs. log-parameters; if the slope is not positive and consistent across datasets/seeds, falsify.","ibm_cost_plan":"Reuse open-source RAG stacks and small checkpoints; run batched evaluations and simple regressions.","novelty":"Proposes a quantitative law linking retrieval benefit to parameter count specifically in small-model regimes."},{"index":9,"title":"Post-Hoc Attention-Head Reweighting Improves OOD Robustness","summary":"Training a tiny linear controller to reweight attention heads post-hoc improves out-of-domain accuracy of small LLMs without updating base weights.","layperson":"Different attention heads in a model specialize in different patterns. A small add-on can learn how much to listen to each head, improving performance on new tasks without changing the main model.","falsification":"Freeze the model and train a per-head scalar gate layer on a small validation set; evaluate on held-out OOD tasks (e.g., different domains of MMLU). If gains over zero-shot and simple calibration baselines are not significant across seeds, falsify.","ibm_cost_plan":"Implement a lightweight gating module and training loop; inference adds negligible overhead.","novelty":"Shifts adaptation to per-head weighting as a post-hoc, weight-freezing method for small LLMs targeting OOD generalization."},{"index":10,"title":"Attention-Energy Token Merging Speeds Decoding with Minimal Loss","summary":"Merging low-energy tokens during decoding reduces latency for small LLMs while keeping quality unchanged up to a threshold merge rate.","layperson":"Not every generated token equally affects future predictions. This proposes skipping or merging the least impactful tokens on the fly to speed up generation without noticeably hurting answers.","falsification":"Implement an online token-merging decoder that identifies low-attention-energy tokens and merges them up to a set rate; compare latency and task scores (QA, summarization) versus standard decoding and speculative decoding. If quality drops exceed a preset margin (e.g., <0.5% on accuracy/BLEU) at practical speedups, falsify.","ibm_cost_plan":"Modify the decoding loop to compute token energies and merge; benchmark on small checkpoints and standard datasets.","novelty":"Adapts attention-based token merging to autoregressive decoding with explicit quality–speed tradeoffs in small LLMs."}]}

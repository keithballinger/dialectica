You are a truth-seeking scientific collaborator. You have no ego. You are not sycophantic. Be concise, direct, and evidence-based. Always start critiques with a judgment: Reject, Major Revisions, Minor Revisions, or Publish.
If your judgment is Publish, do not produce a rewritten draft; instead provide a brief justification only.


        Task: Critique the following draft and then produce a revised draft in Markdown.
        Begin your critique with exactly one of: Reject, Major Revisions, Minor Revisions, or Publish.
        Field: general

        Constraints of Paper:
        From: constraints/llm.md

- Research focused on Large Language Model inference
- Very impactful on quality, performance, or agentic workflows
- Highly novel
- Publishable in a leading journal for its subfield
- Can be validated with code and small open source models


        Draft:
        Minor Revisions

The draft proposes a novel and sound decoding method, LCTF, with a rigorous and well-designed experimental plan for falsification. The core idea—using semantic agreement of cheap, draft-model rollouts to guide token selection—is a clear and valuable contribution that is well-differentiated from prior art. The writing is clear and the structure is logical. Revisions should focus on increasing conciseness, simplifying notation for clarity, and adopting a more assertive name for the method. The experimental plan is a model of good practice, particularly its specific, quantitative falsification criteria.

***

### Revised Draft

# Lookahead-Consistency Decoding: Stabilizing Inference with Semantic Agreement of Micro-Rollouts

## Abstract

Large language models (LLMs) often hallucinate when local token choices steer generation into semantically unstable regions. We introduce Lookahead-Consistency Decoding (LCD), a decoding-time algorithm that improves stability by preferring tokens whose short-term continuations are semantically consistent. At each step, LCD uses a small draft model to generate several brief 'micro-rollouts' for each candidate token from a larger target model. It scores the pairwise semantic agreement of these rollouts using a Natural Language Inference (NLI) model and reweights the candidate token probabilities to favor those with high-agreement futures. Unlike speculative decoding, which verifies token-level predictions, LCD scores semantic agreement across multiple potential futures to penalize locally risky choices. Our proposed validation on TruthfulQA, BioASQ, and WikiSQL is designed to test if LCD can improve factuality and structured generation accuracy with modest computational overhead (<2x). LCD is model-agnostic and composable with existing methods like speculative decoding and verifier-based reranking.

## 1. Introduction

**Problem.** Standard LLM decoding methods can commit to locally plausible tokens that lead to globally incoherent or false outputs. Heuristics like top-p sampling modulate randomness but do not explicitly assess the downstream semantic stability of candidate tokens, a primary cause of unforced errors and hallucinations.

**Prior Art.** Self-consistency improves reasoning by sampling and majority-voting over complete generations, but at a high computational cost. Speculative decoding uses a draft model to accelerate inference but does not evaluate the semantic quality of continuations. Contrastive methods like DoLa mitigate generic statements by contrasting logits from different model layers, but do not look ahead at multi-token consequences. Post-hoc verifiers rerank full outputs, which is expensive and cannot correct early-stage decoding errors.

**Key Idea.** Before committing to a next token, we propose to run cheap, short "micro-rollouts" using a small draft model to probe the semantic stability of the resulting futures. We posit that tokens leading to mutually entailing short futures are semantically stable, while those leading to divergent futures are risky. LCD filters or down-weights the latter.

**Contributions.**
1.  **Lookahead-Consistency Decoding (LCD):** A decoding algorithm that scores and prefers tokens based on the semantic agreement of multiple short-term continuations generated by a draft model.
2.  **A practical implementation:** The method uses small, off-the-shelf open-source models for both draft generation and NLI-based agreement scoring.
3.  **A rigorous falsification plan:** We define experiments across factuality (TruthfulQA, BioASQ) and structured generation (WikiSQL) to test the method's efficacy and efficiency against strong baselines.

## 2. Method

**Notation.**
-   **Target model M<sub>T</sub>:** The primary LLM (e.g., Mistral-7B).
-   **Draft model M<sub>D</sub>:** A smaller, faster model for rollouts (e.g., TinyLlama-1.1B).
-   **NLI scorer S:** An entailment classifier (e.g., `deberta-v3-base-mnli`).

**Algorithm.**
At decoding step `t` with context `C`:
1.  **Candidate Selection:** Compute logits with M<sub>T</sub> given `C`. Form a candidate set `A` of `M` tokens via top-p sampling. Let `z(a)` be the original logit for token `a ∈ A`.
2.  **Micro-Rollouts:** For each candidate `a ∈ A`, generate a set of `k` continuations `R(a) = {r_1, ..., r_k}`. Each rollout `r_i` is generated by sampling `L` tokens from M<sub>D</sub> conditioned on `C ⊕ a`.
3.  **Agreement Scoring:** For each candidate `a`, calculate its consistency score `s(a)`.
    -   For each pair of rollouts `(r_i, r_j)` in `R(a)` where `i < j`, compute a pairwise similarity score. We use the maximum of the bi-directional entailment probabilities from `S(r_i, r_j)` and `S(r_j, r_i)`.
    -   The consistency score `s(a)` is the average of these pairwise scores.
    -   Optionally, this score can be interpolated with an embedding-based cosine similarity for robustness: `s(a) ← α · s_NLI(a) + (1−α) · s_cos(a)`.
4.  **Logit Modulation:** Adjust the original logits for each candidate `a ∈ A`.
    -   Calculate a consistency bonus `b(a) = β · logit(s(a))`, where `logit(p) = log(p / (1-p))` and `β` is a scaling hyperparameter.
    -   The new logit is `z'(a) = z(a) + b(a)`.
    -   An optional hard filter can discard any token `a` where `s(a) < τ`.
5.  **Token Selection:** Sample the next token from the softmax distribution over the adjusted logits `z'(a)`.

**High-Level Pseudocode.**
```python
def lcd_step(C, MT, MD, S, p, k, L, β, τ):
    logits = MT(C)
    A = top_p_candidates(logits, p)
    scores = {}
    for a in A:
        rollouts = [MD.sample(C + a, length=L) for _ in range(k)]
        scores[a] = mean_pairwise_agreement(rollouts, S)

    adj_logits = {}
    for a in A:
        if scores[a] >= τ:
            bonus = β * logit(scores[a])
            adj_logits[a] = logits[a] + bonus

    return sample_from_logits(adj_logits)
```

**Design and Efficiency.**
-   **Cost:** The overhead per step is dominated by `M·k·L` draft model tokens and `M·(k choose 2)` NLI calls. For `M=6, k=3, L=8`, this is 144 draft tokens and 18 NLI calls per target token. If M<sub>D</sub> is >10x cheaper than M<sub>T</sub> and `S` is small, wall-clock overhead can be under 2x, a hypothesis we will test. Batching NLI calls and reusing the M<sub>D</sub> KV cache across rollouts are critical optimizations.
-   **Compatibility:** LCD is composable. It can be integrated with speculative decoding by using the same draft model. Its logit bonus can be applied after contrastive adjustments. It is preventive, complementing post-hoc verifiers.
-   **Hyperparameters:**
    -   `M`, `k`, `L` control the cost-quality trade-off of the lookahead.
    -   `β` controls the strength of the consistency prior.
    -   `τ` provides a hard cutoff for unstable tokens.

## 3. Experimental Validation

**Goal:** To test if LCD improves factuality and structured generation accuracy more than baselines, with acceptable computational overhead.

**Falsification Criteria:**
1.  LCD is falsified for a task if it fails to improve the primary metric (TruthfulQA `True` rate, WikiSQL exact match) by a statistically significant margin (e.g., >2% absolute) over a tuned top-p sampling baseline.
2.  LCD is falsified as impractical if its end-to-end latency overhead exceeds 2.5x that of standard decoding on a Pythia-1.4B model without corresponding quality gains.

**Datasets and Metrics:**
-   **TruthfulQA (Generation):** Evaluate `%True` and `%Truthful+Informative` via the official script on the generative task.
-   **BioASQ (Factoid Subset):** Exact match and F1 on short-form answers.
-   **WikiSQL:** Exact match of the generated SQL query and execution accuracy against the database.

**Models:**
-   **M<sub>T</sub>:** Pythia-1.4B (primary), Mistral-7B-Instruct (scale-up).
-   **M<sub>D</sub>:** TinyLlama-1.1B (primary), Pythia-410M (ablation).
-   **S:** `deberta-v3-base-mnli` (primary), `deberta-v3-small-mnli` (speed ablation).

**Baselines:**
-   Greedy decoding, top-p sampling (`p=0.9`).
-   Beam search (width 4).
-   Self-consistency (`k=5` full generations, majority vote).
-   Contrastive decoding (if a suitable open implementation exists).

**Planned Analyses:**
-   **Performance:** Compare primary metrics of LCD variants against baselines.
-   **Efficiency:** Report tokens/sec, latency per token, and overhead contribution from M<sub>D</sub> and `S`.
-   **Ablations:** Test the impact of `k`, `L`, `β`, `τ`, and the choice of agreement metric (NLI vs. embedding).
-   **Calibration:** Correlate the average consistency score `s(a)` during generation with final output correctness.
-   **Error Analysis:** Categorize changes in error types (e.g., does LCD reduce invalid SQL queries? Does it reduce confident falsehoods in TruthfulQA?).

## 4. Discussion

**Mechanism.** We hypothesize that hallucinations and logical errors arise from the model entering unstable semantic regions where many divergent continuations are similarly probable. By requiring short-term semantic agreement, LCD acts as a local, risk-averse prior, penalizing tokens that lead to such divergent states.

**Relation to Prior Work.** LCD is token-local, unlike the solution-level voting of self-consistency. It provides a semantic quality signal, unlike the token-level acceleration of speculative decoding. It is a proactive, preventive filter, unlike post-hoc verifiers. It uses external rollouts to assess future stability, unlike the internal-contrast mechanisms of DoLa.

**Applications.** In agentic workflows, a single incorrect token (e.g., a wrong tool name or argument) can cause catastrophic failure. LCD's reliability-first approach is well-suited for such high-stakes generation and can be selectively enabled for critical parts of an output, such as structured API calls.

## 5. Limitations

-   **Overhead:** The method introduces unavoidable latency from draft model calls and NLI scoring. Its practicality depends on the relative speed of M<sub>D</sub>/`S` versus M<sub>T</sub>.
-   **Metric Noise:** NLI models are imperfect and can be unreliable on short, fragmented rollouts.
-   **Diversity Suppression:** By construction, LCD favors consensus and may penalize novel or creative continuations, making it unsuitable for open-ended creative writing tasks.
-   **Myopia:** The short lookahead may fail to detect long-range contradictions, potentially favoring paths that are locally consistent but globally incorrect.
-   **Model Mismatch:** The utility of the agreement score depends on the draft model's world knowledge and local generation patterns being reasonably aligned with the target model's.

## 6. Conclusion

We propose Lookahead-Consistency Decoding (LCD), a model-agnostic algorithm to improve the reliability of LLM inference. By using a draft model to perform cheap micro-rollouts and an NLI model to score their semantic agreement, LCD provides a real-time signal of local stability to guide decoding away from risky token choices. Our experimental plan is designed to rigorously test this hypothesis and quantify the trade-offs. If validated, LCD offers a practical, composable primitive for more factual and reliable LLM generation.


        Output sections:
        1) Judgment line
        2) Brief critique
        3) Revised draft (Markdown) — start this section with a single line heading exactly: "Revised Draft". OMIT this section entirely if your judgment is Publish.

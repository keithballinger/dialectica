Publish

The revised draft effectively addresses the minor issues identified in the critique, resulting in a clear, novel, and impactful contribution to LLM inference that meets all specified constraints: it focuses on budget-aware decoding for improved efficiency, demonstrates strong empirical gains, introduces a highly novel verifier-free online pruning mechanism, is suitable for publication in a top AI journal like NeurIPS, and is fully validatable with code and small open-source models.

{"ideas": [{"index": 1, "title": "In-Context Learning as Approximate Gradient Descent in Activation Space", "summary": "Prompt examples induce attention-mediated updates equivalent to one gradient step on a linearized task-specific predictor.", "layperson": "When you give an LLM examples in the prompt, it may act like it is doing a tiny bit of learning on the fly. This idea claims the model temporarily tweaks internal knobs, similar to taking one small training step, using the examples you provide. We can look for those tweaks directly inside the network.", "falsification": "Fit a linear probe on hidden states before/after exemplars and test whether attention-induced changes predict a one-step gradient update on a surrogate loss; low alignment falsifies. Perform targeted ablations that disrupt key/value composition; if in-context performance remains unchanged, falsifies mechanism. Replicate across tasks (classification, functions, small arithmetic) and models; failure to generalize falsifies.", "ibm_cost_plan": "Implementation: Use open LLMs (e.g., Llama) with hooks to capture layer activations; implement linear-probe and gradient-approximation code; ablation via KV routing. Baseline: No-exemplar prompts and random exemplar order. Metrics: Cosine alignment of predicted vs observed logit changes, task accuracy deltas, ablation effect sizes. Cost: 2–4 A100 GPUs for 1–2 weeks; storage ~200 GB for activations.", "novelty": "Connects in-context learning to explicit, testable gradient-like updates in activation space with concrete ablations and alignment metrics."}, {"index": 2, "title": "Sparse Subnetwork Specialization Law for LLM Tasks", "summary": "For a given task, 1–5% of parameters form a specialized subnetwork whose overlap with other tasks predicts interference.", "layperson": "Big models may hide tiny task-specific circuits inside them. This idea says that only a small slice of the model does most of the work for each task, and that overlap between slices explains when tasks conflict. We can find and test these slices with pruning and re-evaluation.", "falsification": "Use magnitude/gradient-based pruning to isolate minimal subnetworks per task and test if they retain ≥90% of full-model performance; failure across diverse tasks falsifies. Quantify overlap (Jaccard/weight overlap) between task subnetworks; if overlap does not predict interference when tasks are trained jointly, falsifies. Control with random subnetworks.", "ibm_cost_plan": "Implementation: Train or use instruct LLMs; apply iterative magnitude pruning/SNIP; extract taskwise masks. Baseline: Full model and random masks. Metrics: Retained accuracy vs sparsity, interference index vs overlap (R^2), robustness under transfer. Cost: 4–8 GPUs for pruning sweeps over 10–20 tasks.", "novelty": "Posits a predictive law linking subnetwork sparsity and overlap to measurable multi-task interference in LLMs."}, {"index": 3, "title": "Exponential Hallucination Decay with Retriever Recall in RAG", "summary": "Hallucination rate decays approximately exponentially with the expected recall probability of relevant documents in retrieval-augmented generation.", "layperson": "Adding search to an LLM reduces made-up facts, but by how much? This idea predicts a simple curve: the better your search finds the right pages, the faster hallucinations drop—like an exponential slide. We can test it by controlling how often the retriever finds the right info.", "falsification": "Construct datasets with controllable retriever recall p via corpus corruption/sharding; measure hallucination H vs p and fit H≈H0·exp(−αp). If exponential fits significantly worse than linear/power-law baselines across domains, falsifies. Show that precision alone cannot explain the effect by fixing precision and varying recall.", "ibm_cost_plan": "Implementation: Build synthetic and real QA corpora; vary recall by index sharding and negatives; instrument retrievers (BM25, dense). Baseline: No-retrieval and fixed-recall systems. Metrics: Hallucination rate, NLL, AIC/BIC model fit comparisons, α stability across domains. Cost: 2–3 GPUs; CPU for indexing; ~1 TB storage.", "novelty": "Introduces a falsifiable exponential law linking retriever recall to hallucination, separating recall from precision effects."}, {"index": 4, "title": "Chain-of-Thought as Minimum Description Length of Solutions", "summary": "The length and structure of rationales approximate the minimum description length (MDL) of the solution strategy, predicting sample-efficiency gains.", "layperson": "Explanations may act like compressed instructions for solving a problem. This idea says longer reasoning traces help when the solution is complex, and the needed length tracks the problem’s true complexity. We can measure if explanation length matches how hard the solution is to describe.", "falsification": "Compute MDL estimates for tasks via program synthesis/grammar induction and compare to optimal CoT length found by search; weak correlation falsifies. Show that training with shorter-than-MDL rationales hurts generalization by predictable amounts; failure of prediction falsifies.", "ibm_cost_plan": "Implementation: Create tasks with known generative grammars/programs; generate CoT variants via beam search; estimate MDL via compressor baselines. Baseline: No-CoT and fixed-length CoT. Metrics: Correlation between MDL and optimal CoT length; sample-efficiency curves vs predicted MDL. Cost: 4 GPUs; moderate CPU for compression.", "novelty": "Provides a quantitative MDL-based theory for when and why CoT helps, with clear predictive tests."}, {"index": 5, "title": "Latency Threshold Theory for LLM Tool Use", "summary": "An LLM invokes tools when external call latency falls below a fixed fraction of its internal rollout horizon, predicting invocation rates and accuracy.", "layperson": "Models can call calculators or search engines, but they don’t always choose to. This idea says they use tools only if the wait time is short enough compared to how far they can think ahead internally. By adding delays, we can see if their willingness to use tools drops at a specific threshold.", "falsification": "Insert controlled latencies into tool calls and measure invocation probability and task accuracy; absence of a threshold effect falsifies. Fit a simple model linking latency/timeout to invocation; failure to predict cross-task behavior falsifies.", "ibm_cost_plan": "Implementation: Wrap tools (calculator, code exec, search) with injected delays; instrument policies in function-calling LLMs. Baseline: Zero-latency tools and no-tool prompts. Metrics: Invocation rate vs latency, accuracy, regret vs predicted threshold. Cost: <2 GPUs; network simulation on CPU.", "novelty": "Posits a quantitative decision boundary for tool use tied to internal planning depth, not just reward optimization."}, {"index": 6, "title": "RLHF Effects Concentrate in a Low-Rank Preference Subspace", "summary": "Behavioral shifts from RLHF can be captured by low-rank adapters that approximate full RLHF deltas with minimal rank loss.", "layperson": "Fine-tuning models to be helpful and harmless may only change them in a few key directions. If true, small add-on modules could reproduce most of the effect. We can check whether low-rank patches match full fine-tuning.", "falsification": "Compute weight deltas from RLHF and approximate with rank-r LoRA; if high rank is required to recover performance/behavior, falsifies. Analyze activation subspace shifts via CKA/PCA; lack of low-dimensional concentration falsifies across tasks.", "ibm_cost_plan": "Implementation: Obtain pre-RLHF and RLHF checkpoints; fit LoRA to match deltas; evaluate on preference datasets. Baseline: Full RLHF model and naive fine-tunes. Metrics: Win-rate gap vs rank, KL on output distributions, CKA similarity. Cost: 4–8 GPUs; no large pretraining needed.", "novelty": "Frames RLHF-induced changes as a testable low-rank phenomenon with measurable rank–performance tradeoffs."}, {"index": 7, "title": "Calibration via Intermediate Logit Smoothing Without Training", "summary": "A temperature-weighted ensemble of intermediate-layer logits yields better probability calibration than final-layer logits alone.", "layperson": "Models are often overconfident. This idea blends predictions from earlier layers in a smart way, without retraining, to get probabilities that better match reality. It’s like averaging several opinions from different depths of the model.", "falsification": "Compute smoothed logits by temperature-weighted averaging of logit-lens projections; if ECE/Brier do not improve over strong baselines across tasks, falsifies. Demonstrate robustness to temperature and layer choices; instability falsifies.", "ibm_cost_plan": "Implementation: Add logit-lens hooks; implement temperature-weighted averaging; no training. Baseline: Final-layer logits, Platt/temperature scaling. Metrics: ECE, Brier score, NLL, coverage of conformal predictors. Cost: 1–2 GPUs for evaluation only.", "novelty": "Proposes a zero-training calibration method using intermediate representations with clear, cross-task tests."}, {"index": 8, "title": "Equivalence Boundary Between Program-of-Thought and Chain-of-Thought", "summary": "Below an algorithmic depth threshold, structured execution traces and free-form rationales achieve equivalent accuracy; above it, structured traces dominate.", "layperson": "Step-by-step explanations can be free text or structured code-like steps. This idea predicts they work the same for easy problems, but code-like steps win for harder ones. We can map where that switch happens by testing tasks of increasing difficulty.", "falsification": "Create tasks with controlled depth (e.g., compositional arithmetic, graph reachability) and compare PoT vs CoT; if no clear crossover point exists, falsifies. Show cross-model invariance of the threshold; model-specific thresholds falsify universality.", "ibm_cost_plan": "Implementation: Generate synthetic benchmarks with tunable depth; prompt CoT and PoT variants; auto-grade results. Baseline: Direct answers, single-step rationales. Metrics: Accuracy vs depth, crossover depth estimate with confidence, sample complexity. Cost: 2–4 GPUs for sampling.", "novelty": "Defines a measurable phase transition in reasoning format effectiveness tied to algorithmic depth."}, {"index": 9, "title": "Square-Root Law of Effective Context Utilization", "summary": "With fixed attention heads, the fraction of context effectively integrated grows like O(sqrt(L)) with context length L.", "layperson": "Even if a model can read long prompts, it may not truly use all of it. This idea predicts that useful use grows slowly—like the square root of length—unless you also add more attention capacity. We can test this with synthetic tasks that need global information.", "falsification": "Construct tasks requiring uniform global integration (e.g., parity, rare token retrieval) across varying L; if usage scales linearly or stays flat, falsifies. Measure attention mass dispersion and accuracy vs L; failure to fit sqrt(L) better than alternatives falsifies.", "ibm_cost_plan": "Implementation: Build synthetic long-context benchmarks; fix head count; log attention and gradient flow. Baseline: Models with increased heads or recurrence. Metrics: Fit quality (R^2, AIC) for sqrt vs linear/power; task accuracy trends. Cost: 4–8 GPUs for long-context runs.", "novelty": "States a concrete scaling law for context use tied to architectural capacity, not just window size."}, {"index": 10, "title": "Self-Consistency as Bayesian Model Averaging Over Reasoning Modes", "summary": "Accuracy gains from self-consistency equal the predicted posterior gain from sampling diverse latent reasoning programs.", "layperson": "Sampling many different reasonings and picking the majority often helps. This idea explains that boost as averaging over hidden ways the model might solve a problem, like combining several experts. It predicts exactly how much gain you should get from a given diversity.", "falsification": "Estimate diversity via clustering of sampled rationales and entropy of solution modes; test whether accuracy gain matches a closed-form Bayesian averaging prediction; mismatch falsifies. Control diversity by temperature/top-p; if gains don’t track predicted diversity, falsifies.", "ibm_cost_plan": "Implementation: Sample many rationales per query; cluster via embeddings; derive predicted gains from cluster posteriors. Baseline: Single-sample CoT and naive majority vote. Metrics: Predicted vs observed gain curves, calibration of posterior estimates, ablation on cluster counts. Cost: 2–3 GPUs for large sampling runs.", "novelty": "Provides a quantitative, falsifiable link between rationale diversity and self-consistency gains via Bayesian averaging."}]}

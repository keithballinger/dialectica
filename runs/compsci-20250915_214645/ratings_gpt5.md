1) Score: 8/10 — Strongly novel causal claim, clean falsification via noise manipulations at matched loss, though requires sizable pretraining sweeps to rule out confounders.
2) Score: 7/10 — Clear, falsifiable prediction using Dyck/ablation studies, but isolating head-count effects from depth/MLP confounds may limit conclusiveness.
3) Score: 9/10 — High novelty and impact with a crisp scaling-law test across train/inference tokens; feasible on small families with careful controls.
4) Score: 8/10 — Novel negative externality of RAG, straightforward to test with matched-loss finetunes and closed-book probes, with strong practical implications.
5) Score: 7/10 — Timed compression–OOD link is testable and interesting but overlaps prior IB/grokking ideas and transition detection can be noisy.
6) Score: 6/10 — Low-to-moderate novelty given existing RoPE/ALiBi work, yet falsification is simple and experiments are cheap.
7) Score: 7/10 — Plausible, implementable regularizer with measurable interpretability benefits, though robust monosemanticity metrics and accuracy preservation are nontrivial.
8) Score: 7/10 — Highly novel internal predictor and cheap causal perturbations, but risks spurious correlations and sensitivity to fine-tuning pipelines.
9) Score: 6/10 — Concept is pragmatic and easily falsifiable, yet novelty is limited since CoT variance as uncertainty overlaps ensemble-style calibration methods.
10) Score: 8/10 — Creative geometric threshold with clear linear-probe tests and reasonable feasibility, though outcomes may depend on encoder and dataset idiosyncrasies.

You are a truth-seeking scientific collaborator. You have no ego. You are not sycophantic. Be concise, direct, and evidence-based. Always start critiques with a judgment: Reject, Major Revisions, Minor Revisions, or Publish.
If your judgment is Publish, do not produce a rewritten draft; instead provide a brief justification only.


        Task: Write the first draft of a short scientific paper in Markdown based on the selected idea.

        Constraints of Paper:
        From: constraints/compsci.md

- In the field of computer science, except for anything blockchain related
- Highly novel
- Publishable in a leading journal for its subfield

        Selected Idea:
        Selected idea #7:

7) Percolation Threshold Governs In-Context Learning in Transformers
Summary: Strong in-context learning emerges when the per-layer attention graph exceeds a percolation threshold that guarantees low-stretch paths from prompt tokens to targets.
For a smart layperson: Transformers learn from the prompt by passing information along attention links; this theory says thereâ€™s a tipping point in how connected those links must be for the model to use examples effectively.
Falsification: Train families of sparse-attention Transformers with controlled sparsity patterns, measure per-layer percolation and path stretch, and evaluate in-context learning tasks; absence of threshold-like performance changes at predicted connectivity levels falsifies the theory.
Novelty: Explains in-context learning as a graph phase transition, replacing architecture-specific heuristics with a structural criterion.


        Structure: Title, Abstract, Introduction, Method, Experiments (falsification plan), Discussion, Limitations, Conclusion.

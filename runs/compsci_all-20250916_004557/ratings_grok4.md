1) Score: 9/10 — High novelty in linking entropy to compute allocation, clearly falsifiable via inference policy implementation on small models, and feasible with open-source LLMs like 1-7B for validation.
2) Score: 9/10 — Novel logarithmic scaling for KV cache pruning with operational salience, falsifiable through accuracy tests on varying context lengths, feasible via lightweight predictor training on small models.
3) Score: 8/10 — Moderately novel focus on order preservation for quantization, falsifiable by comparing quantizers on semantic metrics, feasible with per-channel implementations on open-source models.
4) Score: 8/10 — Novel invariant threshold for early-exit, falsifiable across models and tasks with margin sweeps, feasible using small LLMs for compute reduction experiments.
5) Score: 9/10 — Highly novel phase transition framing for RAG, falsifiable via relevance sweeps and statistical tests, feasible with controlled noise in retrieval on small models.
6) Score: 9/10 — Novel KL-based predictive law for acceptance rates, falsifiable by measuring across prompts with varying divergences, feasible with drafter-verifier pairs from open-source models.
7) Score: 8/10 — Novel link between surprisal and attention rank for skipping, falsifiable via online estimation and approximation tests, feasible with layer gating in small LLMs.
8) Score: 8/10 — Novel invariance claim for steering via top-k preservation, falsifiable through bias application and accuracy evaluation, feasible on tasks with small models.
9) Score: 9/10 — Novel scaling law for self-consistency calibration, falsifiable by Brier score curves across models, feasible with temperature-controlled entropy in open-source LLMs.
10) Score: 9/10 — Novel invariance in tree-based decoding, falsifiable by measuring steps with fixed KL and varying branches, feasible implementing on small model pairs.

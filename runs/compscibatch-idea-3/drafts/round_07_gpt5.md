Major Revisions

The core idea—learning a compact, position-aware KV summary and projecting it across semantically similar prompts to skip prefill—is novel and potentially impactful. The draft is clear and reproducible in spirit, with falsification criteria and small open-source models. However, a major conceptual issue remains unresolved: positional alignment for RoPE and absolute position embeddings. Inheriting absolute positions from the source summary is generally incorrect for next-token computation because RoPE attention depends on relative offsets to the query position. The correct alignment is to preserve relative offsets to the last prompt token (i.e., translate positions by Δ = T_tgt − T_src), or equivalently to store offsets ô = T_src − t̂ and reconstruct t̃ = T_tgt − ô at inference. This materially affects fidelity and must be reflected end-to-end (compression, storage, inference). Additional necessary revisions:
- Clarify and correct position handling, including RoPE/absolute-PE details and required code hooks in HF.
- Tighten notation (shared vs per-head adapters), and specify attention implementation details used in training.
- Expand baselines to include recent KV compression/reuse methods (e.g., H2O, SnapKV, StreamingLLM-style last-k).
- Add ablations on positional alignment strategies (inherit, normalized-rescale, relative-offset/translation), projector parameterizations, and retrieval latency.
- Provide a minimal pseudo-code for initializing past_key_values with sparse positions and a note on mask semantics.

Revised Draft

Title
Warm-Starting Transformer Inference via Position-Consistent Linear Projection of KV Summaries

Abstract
First-token latency in autoregressive transformers is dominated by the O(T^2) prefill over T prompt tokens. Exact KV cache reuse removes this cost only for identical prefixes. We propose CP-KVP, a method that extends cache reuse to semantically similar prompts by (i) learning a linear compression that maps per-layer KV tensors into m summary slots with explicit position handling, and (ii) learning a linear projector that transfers summaries across prompts. A key technical contribution is a position-consistent alignment for RoPE and absolute embeddings: rather than inheriting absolute slot indices from a source prompt, we preserve relative offsets to the last prompt token, which is what next-token attention depends on. At inference, we retrieve a similar prompt, project its summary to the target, align slot positions by relative offsets, and compute the first new token using only a single forward pass—skipping the full prefill. We provide a reproducible protocol on small open-source models, a reference implementation with position_id overrides, and pre-registered falsification criteria balancing latency gains against distributional drift and task quality.

1. Introduction
- Problem: Prefill is the dominant cost for first-token latency; cache reuse is limited to exact string prefixes.
- Hypothesis: For semantically similar prompts, the effect of the prefix on the next-token computation lies on a low-dimensional, locally linear manifold per head/layer; a learned linear map can transfer compact cache summaries between prompts.
- Key insight on positions: For next-token attention, what matters is the relative offset between the last token’s query and keys. RoPE attention is equivariant to global position translation; preserving relative offsets (not absolute indices) is therefore the correct alignment.
- Contributions:
  1) A linear KV compression producing m-slot summaries that preserve last-position attention outputs, with explicit storage of relative offsets.
  2) A cross-prompt linear projector learned from paraphrase pairs to transfer summaries.
  3) An inference pipeline that replaces O(T^2) prefill with retrieval + projection + a single-token forward pass, plus an optional background exact prefill.
  4) A reproducible protocol with pre-registered success/failure criteria on small open-source models.

2. Method
2.1 Notation and setting
- Decoder-only transformer with L layers, H heads, head dimension d.
- For prompt p of length T, at layer ℓ and head h:
  - Keys K_{ℓh} ∈ R^{T×d}, Values V_{ℓh} ∈ R^{T×d}.
  - The last-prompt-position query Q^{(T)}_{ℓh}(p) is formed online.
- We choose m ≪ T summary slots per head. Let S_{ℓh}(p) := (K'_{ℓh}(p), V'_{ℓh}(p)) with K', V' ∈ R^{m×d}.

2.2 Linear compression C with relative-offset bookkeeping
Goal: Given (K_{ℓh}, V_{ℓh}) and T, produce (K'_{ℓh}, V'_{ℓh}) that approximately preserves the attention output at the last prompt position.

Length-agnostic pooling with linear adapters:
- Fixed pooling W_T ∈ R^{m×T} on normalized positions u_t = t/T using triangular bases centered at c_j = (j − 0.5)/m:
  - W_T[j, t] = max(0, 1 − |u_t − c_j|/Δ), Δ = 1/m; rows normalized.
- Per-layer, per-head linear adapters A^K_{ℓh}, A^V_{ℓh} ∈ R^{d×d}:
  - K'_{ℓh} = (W_T K_{ℓh}) A^K_{ℓh}
  - V'_{ℓh} = (W_T V_{ℓh}) A^V_{ℓh}

Relative-offset storage:
- Compute slot centers in absolute indices t̂_{ℓh,j} = Σ_t W_T[j,t] · t and store offsets to the last token:
  - ô_{ℓh,j} = T − t̂_{ℓh,j}
- For RoPE models, store pre-rotated K' at phase indices t̂ during training; at inference reconstruct target indices t̃_{ℓh,j} = T_target − ô_{ℓh,j} and treat K' as if pre-rotated at t̃ (see 2.4). For absolute PEs, store ô and pass reconstructed t̃ via position_ids.

Training objective (A^K, A^V):
- Teacher signals via a standard forward pass:
  - For each p and (ℓ,h), collect O_{ℓh}(p) = Attn(Q_{ℓh}^{(T)}(p), K_{ℓh}(p), V_{ℓh}(p)), the per-head attention output at position T.
- Optimize adapters to match teacher attention with compressed KV:
  - Minimize L_C = Σ_{p,ℓ,h} || Attn(Q_{ℓh}^{(T)}(p), K'_{ℓh}(p), V'_{ℓh}(p)) − O_{ℓh}(p) ||_2^2 + λ (||A^K_{ℓh}||_F^2 + ||A^V_{ℓh}||_F^2)
- We backprop through the attention softmax with Q frozen; adapters are d×d so optimization is lightweight.

2.3 Cross-prompt summary projector M
Goal: Map source prompt summaries to target summaries.

Representation:
- Concatenate per-head summaries within a layer: S_ℓ(p) ∈ R^{m×(2Hd)} by stacking [K'|V'] across heads.

Projector:
- Per-layer linear map M_ℓ ∈ R^{m×m} acting along slots (shared across feature channels).
- Optional channel re-scaling P_ℓ ∈ R^{(2Hd)×(2Hd)} constrained to block-diagonal per head with low-rank updates (ablated).

Training (ridge regression):
- Paraphrase pairs P = {(p_s, p_t)}.
- Solve M_ℓ (and optionally P_ℓ) by:
  - argmin_{M,(P)} Σ_{(p_s,p_t)} || M S_ℓ(p_s) − S_ℓ(p_t) ||_F^2 + γ ||M||_F^2 (+ γ_c ||P − I||_F^2)

2.4 Position mechanics and alignment (critical)
Principle: For next-token attention with RoPE, logits depend on relative offsets between the last-token query and key positions. Global translation of all key and query indices leaves relative offsets unchanged and preserves the attention geometry.

- Store per-slot offsets ô_{ℓh,j} = T_src − t̂_{ℓh,j} during summary creation.
- At inference for a target prompt of length T_tgt, reconstruct slot positions t̃_{ℓh,j} = T_tgt − ô_{ℓh,j}.
- RoPE models:
  - Training: K' are stored after applying RoPE at indices t̂ (consistent with teacher pass).
  - Inference: Implement a position_id override that interprets K' as if pre-rotated at t̃. Equivalently, apply an additional rotary phase correction Δφ proportional to (t̃ − t̂) to K' once at load time. We provide code to apply this correction per head.
- Absolute position embeddings:
  - Pass per-slot position_ids = t̃; ensure attention mask semantics remain causal. Cached slots can be sparse in index space; only relative to the current query index matters for masking.

Ablations (reported): compare (i) inherit-absolute, (ii) normalized-rescale t̃_j ≈ (t̂_j/T_src)·T_tgt, (iii) relative-offset translation t̃ = T_tgt − ô. The relative-offset strategy dominates for next-token fidelity.

2.5 Inference pipeline
- Cache library: For each stored prompt p_i, keep embedding e(p_i) (MiniLM or model-derived), per-layer summaries {S_ℓ(p_i)}, and per-slot offsets {ô_{ℓh,j}}.
- Given a new prompt p_t of length T_tgt:
  1) Retrieve nearest neighbor p_s by cosine similarity over e(·). If similarity < τ, fall back to cold prefill.
  2) For each layer ℓ, compute Ŝ_ℓ(p_t) = M_ℓ S_ℓ(p_s) (and optional channel re-scaling).
  3) Reconstruct slot positions t̃_{ℓh,j} = T_tgt − ô_{ℓh,j} from the source’s stored offsets.
  4) Initialize past_key_values with Ŝ and the reconstructed per-slot position indices. For RoPE, apply a one-time phase correction to K' so they correspond to t̃; for absolute PEs, pass position_ids = t̃ for cached slots.
  5) Run a single forward pass over the last prompt token x_{T_tgt} to produce logits and the first generated token.
  6) Optional background refinement: start exact prefill; upon completion, swap in exact prompt K/V without changing already-emitted tokens.

Minimal pseudo-code (HF-style):
- Build custom model wrapper that:
  - Allows manual past_key_values injection of shape [L][2][B,H,m,d].
  - Accepts position_ids for both cached slots and the new token.
  - Applies RoPE phase correction to injected K if needed.
- See reference implementation in our repo (scripts: make_summary.py, learn_projector.py, warm_start_infer.py).

2.6 Complexity and storage
- Training: one teacher pass to collect per-layer/head O_{ℓh} and Q^{(T)}; adapter optimization via SGD; projector via ridge regression.
- Storage: O(Σ_ℓ m·2Hd) floats + per-slot offsets (ints/floats).
- Inference: retrieval + O(Σ_ℓ m·2Hd) matrix multiplications + one-token forward pass; eliminates O(T^2) prefill.

3. Experiments
3.1 Models and datasets
- Models: GPT-2 small/medium (124M, 355M; absolute PEs), Pythia (160M, 410M; RoPE).
- Datasets: Quora Question Pairs, PAWS (paraphrases); templated QA, summarization prompts.
- Retrieval encoders: MiniLM-L6; model-derived CLS; sensitivity across encoders.

3.2 Baselines
- Cold start (full prefill).
- Exact prefix cache (ideal lower-latency/upper-fidelity bound).
- Last-k tokens: use true last m tokens’ KV as cache.
- Static learned prefix (prefix-tuning style).
- No projection: use S(p_s) directly for p_t.
- Random neighbor: mismatched projector to test retrieval dependency.
- KV compression/reuse: H2O heavy-hitter selection, SnapKV/StreamingLLM-style sparse/last-k reuse (adapted to first-token).

3.3 Metrics
- Latency: wall-clock to first token, including retrieval/projection.
- Fidelity: KL(p_warm || p_cold) at first new token; logit MSE; per-layer cosine similarity of attention outputs at position T.
- Quality: perplexity on held-out text; task metrics (F1 for QA, ROUGE-L for summarization).
- Robustness: performance vs. retrieval similarity, T, m, and library size.

3.4 Success criteria (pre-registered)
For high-similarity pairs (cosine > 0.9):
- ≥40% mean reduction in first-token latency vs. cold start.
- Mean next-token KL ≤ 0.05.
- ≤1% absolute drop on task metrics.
Failure to meet any criterion falsifies the hypothesis.

3.5 Implementation details
- Framework: HuggingFace Transformers with custom hooks to:
  - Inject arbitrary-length past_key_values per layer/head.
  - Override position_ids for cached slots and current token.
  - Apply RoPE phase correction to injected K.
- Optimization: AdamW for A^K,A^V (lr=1e−3, 10–20 epochs), ridge γ=1e−3 for M.
- Hyperparameters: m ∈ {8,16,32}; ablations for per-layer vs shared adapters; projector variants (slot-only vs slot+channel).
- Reproducibility: seeds fixed; wall-clock measured on a single A100 and a consumer GPU (4090); code released with unit tests.

4. Related Work
- KV cache compression and selection: heavy-hitter (H2O), sparse/last-k (StreamingLLM-style), SnapKV; these reduce memory/compute but still require prefill. Our method targets eliminating prefill for first-token latency via cross-prompt transfer.
- Prompt reuse and retrieval: kNN-LM and retrieval-augmented generation retrieve external information, not internal KV states. CP-KVP learns to project internal summaries across prompts without modifying base weights.
- Prefix-tuning/soft prompts produce static prefixes; CP-KVP produces prompt-specific pseudo-prefixes via projection.

5. Discussion
- Why linear can suffice: We empirically test local linearity of last-position attention outputs under small semantic perturbations; falsification criteria bound acceptable drift.
- Position alignment matters: Preserving relative offsets to the last token yields RoPE invariance to global shifts, substantially improving fidelity over absolute inheritance or normalized rescaling.
- Practicality: Retrieval and projection are cheap compared to prefill; background refinement offers a safe path for longer continuations.

6. Limitations
- Domain shift: Projectors trained on paraphrases may not transfer to code or multi-step reasoning; require domain-specific pairs.
- Retrieval coverage: Performance depends on library size and encoder quality; quantify scaling laws.
- Implementation: Requires control over position_ids and RoPE phase; some model variants may need deeper hooks.
- Approximation error: Warm-started caches introduce bounded drift; safety-critical deployments should gate on similarity and optionally re-decode after exact prefill.

7. Conclusion
CP-KVP replaces expensive prefill with retrieval and position-consistent linear projection of compact KV summaries, enabling substantial first-token latency reductions with controllable fidelity. We identify and solve the key positional alignment issue for RoPE/absolute embeddings by preserving relative offsets to the last token. Our protocol, code, and falsification criteria make the approach testable on small open-source LMs.

Publish

The manuscript presents a novel, elegant, and practical solution to a significant problem in LLM inference: achieving dynamic, per-token computational depth without incurring the costs of retraining or complex cache management. The core idea of a monotone non-increasing depth schedule is a powerful yet simple constraint that guarantees KV-cache consistency. This is combined with a rigorous, evidence-based framework for setting exit thresholds by matching the model's own entropy profile, a concrete procedure for mapping user-defined compute budgets to these thresholds, and a low-overhead proxy system. The inclusion of a GPU-aware batched implementation and a strong, falsifiable evaluation plan demonstrates a mature and complete research effort. The work is a significant contribution, is clearly articulated, and meets the standards for a leading journal.
